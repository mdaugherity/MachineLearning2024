# Machine Learning 2024
**PHYS 453 - Spring 2024, Dr. Mike Daugherity, Abilene Christian University.**

## Intro to Python and Data Science Tools
* [PPT 0 - First Day slides](pdf/ML0.pdf): A too-short intro to data science and machine learning, and why it is taught by a physicist
* [Tutorial 1 - Python Cheat Sheet](tutorial/Tutorial_01_Python_Cheat_Sheet.ipynb):  Reminders of logic, loops, and formatting output 
* [Tutorial 2 - Numpy](tutorial/Tutorial_02_Numpy.ipynb): Doing math with arrays in [numpy](https://numpy.org/)
* [Tutorial 3 - matplotlib](tutorial/Tutorial_03_Plots_with_matplotlib.ipynb): Making plots with [matplotlib](https://matplotlib.org/)
* Cheat Sheets: [numpy](pdf/numpy_cheat_sheet.pdf) | [matplotlib](pdf/matplotlib_cheat_sheet.pdf) | [pandas](pdf/pandas_cheat_sheet.pdf) | [sklearn](pdf/sklearn_cheat_sheet.pdf)
* [Class Notes 1-22-24](class/Class_1_22_24_Intro.ipynb): Examples from class


## Machine Learning Basics
* [Tutorial 4 - First_Look_at_Data](tutorial/Tutorial_04_First_Look_at_Data.ipynb): Using scikit-learn and pandas to look at real data
* [PPT 1 - Intro To Classifiers](pdf/ML1.pdf): The basic vocabulary of supervised learning and classifiers 
* [Homework 1 - DIY_1D_Classifier](class/HW1_DIY_1D_Classifier.ipynb) | [HW1 Template](class/HW1_Template.ipynb): Make your own simple classifier!  Make that machine learn!
* [Homework 2 - Won't You Be My Neighbor](class/HW2_Neighbors.ipynb) | [HW2 Template](class/HW2_Template.ipynb): Make a simple Nearest Neighbor classifier

## Classification Tools
* [PPT 2 - Evaluating Classifiers](pdf/ML2.pdf) | [Code Example](class/Metrics_examples.ipynb): Notes on how to score a classifier.  With Math!
* [Tutorial 5 - A_Classifier_Class](tutorial/Tutorial_05_A_Classifier_Class.ipynb): Lets give our classifier an interface like the scikit-learn classifiers
* [Tutorial 6 - Tuning and Evaluation](tutorial/Tutorial_06_Tuning_and_Evaluation.ipynb): How to tune and evaluate performance of a classifier
* [Tutorial 7 - Data Transforms and Pipelines](tutorial/Tutorial_07_Transforms_and_Pipelines.ipynb): The unskippable step of scaling data and why pipelines are great
* [Tutorial 8 - First Classification Recipe](tutorial/Tutorial_08_First_Classification_Recipe.ipynb): Putting the pieces together into usable code!

## Classifier Deep Dives
For most classifiers I have Powerpoint slides explaining how it works and tutorial code for how to use it.
* Nearest Neighbors:  [PPT 3](pdf/ML3.pdf) | [Tutorial 9](tutorial/Tutorial_09_Nearest_Neighbors.ipynb)
* Decision Trees: [PPT 4](pdf/ML4.pdf) | [Tutorial 10](tutorial/Tutorial_10_Decision_Trees.ipynb)
* Bayesian Classifier: [PPT 5](pdf/ML5.pdf) | [Tutorial 12](tutorial/Tutorial_12_Bayesian_Classifier.ipynb)
* Linear Models: [PPT 6](pdf/ML6.pdf)
* Neural Networks: [PPT 7](pdf/ML7.pdf) | [Tutorial 14](tutorial/Tutorial_14_Neural_Networks.ipynb)
* Gradient Boosted Trees: [PPT 9](pdf/ML9.pdf) | [Tutorial 15](tutorial/Tutorial_15_Boosted_Trees.ipynb)
* Deep Learning: [PPT 11](pdf/ML11.pdf) | [Tutorial 16](tutorial/Tutorial_16_Deep_Learning_Intro.ipynb) 

## Data Sets
* [Tutorial 11 - Titanic Pandas](tutorial/Tutorial_11_Titanic_Pandas.ipynb): Using pandas to look at the Titanic dataset
* [Homework 3 - Trees on the Titanic](class/HW3_Trees_on_the_Titanic.ipynb) | [HW3 Template](class/HW3_Template.ipynb): Find out who sinks and who swims on real Titanic data
* [Tutorial 13 - Digits Dataset](tutorial/Tutorial_13_Digits_Dataset.ipynb): Exploring the digits dataset for HW4
* [Homework 4 - Digits Throwdown](class/HW4_Digits_Throwdown.ipynb): Show off each classifier's special tricks on the digits dataset
* [Homework 5 - Neural Network OCR](class/HW5_Neural_Network_OCR.ipynb): Train a neural network to read your own handwriting!

## Recipes
Copy-and-paste code to get you started on a problem
* [Classification Recipe](tutorial/Classification_Recipe.ipynb): Code to get you started on a classification problem using pipelines and grid searches
* [Regression Recipe](tutorial/Regression_Recipe.ipynb): Regression problems using pipelines and grid searches (doesn't include discussion on cleaning data)

## Extra Topics
* [PPT 8 - Regression](pdf/ML8.pdf) - Predicting a real-valued number instead of a category
* [PPT 9 - Ensembles](pdf/ML9.pdf) - Improving performance with multiple classifiers/regressors: bagging, adaboost, random forests, gradient boosted trees, XGBoost
* [PPT 10 - Unsupervised Learning](pdf/ML10.pdf) - what we can learn without labeled training data: clustering, genetic evolution, random (stochastic) methods
* [PPT 11 - Modern Methods](pdf/ML11.pdf) - what's new in machine learning: GANS, Stable Diffusion, Deep Learning, LLMs
* 

<!---
COMMENTS!!!!!
* [Classifier Challenge](Class_Classifier_Challenge.ipynb): Evaluating classifiers problem in class 2.22.23
* [Class Challenge](Class_Challenge_03_20_23.ipynb): Spring Break is over, let's remember how to do machine learning
--->

